{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f45b397b",
   "metadata": {},
   "source": [
    "# ETL Project: Technical Report\n",
    "### World Development Indicators in PostgreSQL\n",
    "\n",
    "## Overview\n",
    "\n",
    "**Team Members:**\n",
    "* Hai Pham\n",
    "* Stephen Eldridge\n",
    "\n",
    "**Data Sets:**<br>\n",
    "Data is sourced from http://wdi.worldbank.org/table as CSV files. A total of 3 data sets are used:\n",
    "* “Size of the economy” (wv1.csv)\n",
    "* “Global goals: ending poverty and improving lives” (wv2.csv)\n",
    "* “Global goals: promoting sustainability” (wv3.csv) \n",
    "\n",
    "**ETL Objectives:**<br>\n",
    "We were interested in the data presented in the World Bank’s World Development Indicators series of data sets. In particular, we thought it would be interesting for someone to analyze the relationships between different indicators used in the various data sets. To this end, we decided to take a few of the data sets and perform ETL to create a relational database that would allow indicators from different data sets to be compared and analyzed in future projects.\n",
    "\n",
    "Our method for accomplish this was:\n",
    "* Create a new PostgreSQL database in pgAdmin called world_development_indicators with 3 tables: economy_size, ending_poverty, promoting_sustainability\n",
    "* Extract all 3 CSV files to a different Pandas dataframe in a Jupyter Notebook.\n",
    "* Transform each dataframe to match the schema for the PostgreSQL world_development_indicator_db database.\n",
    "* Load the transformed dataframes to PostgreSQL world_development_indicator_db using the Pandas to_sql function in the Jupyter Notebook.\n",
    "* Run a SQL query in pgAdmin to combine fields from each of the tables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f23e43d",
   "metadata": {},
   "source": [
    "## Extract\n",
    "\n",
    "We acquired the three CSV files for our data set from http://wdi.worldbank.org/table. In a Jupyter Notebook, we used the Pandas read_csv function to import them into dataframes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7d4296",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read economic data data file\n",
    "\n",
    "wv1 = pd.read_csv(\"Resources\\WV1.csv\",header=[0,3])\n",
    "wv1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f722c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read poverty data file\n",
    "\n",
    "wv2 = pd.read_csv(\"Resources\\WV2.csv\",header=[0,3])\n",
    "wv2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232993a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read sustainability data file\n",
    "\n",
    "wv3 = pd.read_csv(\"Resources\\WV3.csv\",header=[0,3])\n",
    "wv3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb117a10",
   "metadata": {},
   "source": [
    "## Transform\n",
    "\n",
    "\n",
    "### Headings\n",
    "\n",
    "In each file, the column headings were spread over multiple rows. We combined the two most salient rows into our headings. We then formatted column headings to be more amenable to a SQL database, replacing spaces with underscores and converting to lowercase. Finally, Certain columns needed to be renamed due to inconsistencies in the import from the CSV and minor issues with incompatible characters.\n",
    "\n",
    "See the example code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d14be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv1.columns=wv1.columns.map(\"_\".join)\n",
    "wv1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258df4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean header names\n",
    "\n",
    "wv1.columns = [x.lower().replace(\" \",\"_\").replace(\",\",\"\") for x in wv1.columns]\n",
    "                    \n",
    "wv1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0089c124",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv1_df=wv1.rename(columns ={\"unnamed:_0_level_0_unnamed:_0_level_1\":\"country\",\"unnamed:_7_level_0_2019\":\"ppp_gni_per_capita_2019\",\\\n",
    "                          \"gross_domestic_product_2019\":\"gross_domestic_product_growth_2019\", \"unnamed:_9_level_0_2019\":\"per_capita_gdp_growth_2019\"})\n",
    "wv1_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bd62c0",
   "metadata": {},
   "source": [
    "### Data cleanup\n",
    "\n",
    "Each file contained empty \"notes\" rows in the original CSV, which we dropped from our final dataframes. We also replaced invalid data and characters within our cells to ensure the string values could later be converted to float values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06440d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop empty rows\n",
    "\n",
    "wv1_df.drop(wv1.index[227:400],inplace=True)\n",
    "wv2_df.drop(wv2.index[226:400],inplace=True)\n",
    "wv3_df.drop(wv3.index[226:400],inplace=True)\n",
    "\n",
    "wv1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2ac834",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop invalid cell data\n",
    "\n",
    "wv1_df1=wv1_df.replace(to_replace=\"..\", value=\"NaN\")\n",
    "wv2_df1=wv2_df.replace(to_replace=\"..\", value=\"NaN\")\n",
    "wv3_df1=wv3_df.replace(to_replace=\"..\", value=\"NaN\")\n",
    "\n",
    "wv1_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a4e504",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv1_df2=wv1_df1.replace(\",\", \"\", regex=True)\n",
    "wv2_df2=wv2_df1.replace(\",\", \"\", regex=True)\n",
    "wv3_df2=wv3_df1.replace(\",\", \"\", regex=True)\n",
    "\n",
    "wv3_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a545e6a",
   "metadata": {},
   "source": [
    "### Data consistency\n",
    "\n",
    "To ensure the data entered into the tables could be compared and analyzed accurately, we converted all figures that had been listed in thousands, millions, billions etc. to absolute values. All percentages and rates per thousand or hundred thousand were converted to decimal values. The resulting numbers were all listed as floats, rounded to three decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be07fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv1_df2['population_2019'] = round(1000000 * wv1_df2['population_2019'].astype(float), 3)\n",
    "wv1_df2['surface_area_2019'] = round(1000 * wv1_df2['surface_area_2019'].astype(float), 3)\n",
    "wv1_df2['population_density_2019'] = round(wv1_df2['population_density_2019'].astype(float), 3)\n",
    "wv1_df2['gross_national_income_atlas_method_2019'] = round(1000000000 * wv1_df2['gross_national_income_atlas_method_2019'].astype(float), 3)\n",
    "wv1_df2['gross_national_income_per_capita_atlas_method_2019'] = round(wv1_df2['gross_national_income_per_capita_atlas_method_2019'].astype(float), 3)\n",
    "wv1_df2['purchasing_power_parity_gross_national_income_2019'] = round(1000000000 * wv1_df2['purchasing_power_parity_gross_national_income_2019'].astype(float), 3)\n",
    "wv1_df2['ppp_gni_per_capita_2019'] = round(wv1_df2['ppp_gni_per_capita_2019'].astype(float), 3)\n",
    "wv1_df2['gross_domestic_product_growth_2019'] = round(0.01 * wv1_df2['gross_domestic_product_growth_2019'].astype(float), 3)\n",
    "wv1_df2['per_capita_gdp_growth_2019'] = round(0.01 * wv1_df2['per_capita_gdp_growth_2019'].astype(float), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fbe1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv2_df2['percentage_share_of_income_or_consumption_2007_18'] = round(0.01 * wv2_df2['percentage_share_of_income_or_consumption_2007_18'].astype(float), 3)\n",
    "wv2_df2['prevalence_of_child_malnutrition_2020'] = round(0.01 * wv2_df2['prevalence_of_child_malnutrition_2020'].astype(float), 3)\n",
    "wv2_df2['maternal_mortality_ratio_2017'] = round(0.00001 * wv2_df2['maternal_mortality_ratio_2017'].astype(float), 3)\n",
    "wv2_df2['under_five_mortality_rate_2019'] = round(0.001 * wv2_df2['under_five_mortality_rate_2019'].astype(float), 3)\n",
    "wv2_df2['incidence_of_hiv_ages_15_49_per1000_2020'] = round(0.001 * wv2_df2['incidence_of_hiv_ages_15_49_per1000_2020'].astype(float), 3)\n",
    "wv2_df2['incidence_of_tuberculosis_2019'] = round(0.00001 * wv2_df2['incidence_of_tuberculosis_2019'].astype(float), 3)\n",
    "wv2_df2['mortality_caused_by_road_traffic_injury_2016'] = round(0.00001 * wv2_df2['mortality_caused_by_road_traffic_injury_2016'].astype(float), 3)\n",
    "wv2_df2['primary_completion_rate_2018'] = round(0.01 * wv2_df2['primary_completion_rate_2018'].astype(float), 3)\n",
    "wv2_df2['vulnerable_employment_2019'] = round(0.01 * wv2_df2['vulnerable_employment_2019'].astype(float), 3)\n",
    "wv2_df2['contributing_family_workers_and_own_account_workers_female'] = round(0.01 * wv2_df2['contributing_family_workers_and_own_account_workers_female'].astype(float), 3)\n",
    "wv2_df2['labor_productivity_growth_2015_18'] = round(0.01 * wv2_df2['labor_productivity_growth_2015_18'].astype(float), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3796fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv3_df2['people_using_safely_managed_drinking_water_services_2017'] = round(0.01 * wv3_df2['people_using_safely_managed_drinking_water_services_2017'].astype(float), 3)\n",
    "wv3_df2['people_using_safely_managed_sanitation_services_2017'] = round(0.01 * wv3_df2['people_using_safely_managed_sanitation_services_2017'].astype(float), 3)\n",
    "wv3_df2['access_to_electricity_2017'] = round(0.01 * wv3_df2['access_to_electricity_2017'].astype(float), 3)\n",
    "wv3_df2['renewable_energy_consumption_2015'] = round(0.01 * wv3_df2['renewable_energy_consumption_2015'].astype(float), 3)\n",
    "wv3_df2['expenditures_for_rd_2015'] = round(0.01 * wv3_df2['expenditures_for_rd_2015'].astype(float), 3)\n",
    "wv3_df2['urban_population_living_in_slums_2014'] = round(0.01 * wv3_df2['urban_population_living_in_slums_2014'].astype(float), 3)\n",
    "wv3_df2['ambient_pm2_5_air_pollution_2016'] = round(wv3_df2['ambient_pm2_5_air_pollution_2016'].astype(float), 3)\n",
    "wv3_df2['adjusted_net_savings_2017'] = round(0.01 * wv3_df2['adjusted_net_savings_2017'].astype(float), 3)\n",
    "wv3_df2['carbon_dioxide_emissions_2014'] = round(wv3_df2['carbon_dioxide_emissions_2014'].astype(float), 3)\n",
    "wv3_df2['nationally_protected_terrestrial_and_marine_areas_2018'] = round(0.01 * wv3_df2['nationally_protected_terrestrial_and_marine_areas_2018'].astype(float), 3)\n",
    "wv3_df2['intentional_homicides_2015'] = round(0.00001 * wv3_df2['intentional_homicides_2015'].astype(float), 3)\n",
    "wv3_df2['internet_use_2017'] = round(0.01 * wv3_df2['internet_use_2017'].astype(float), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c13ffd",
   "metadata": {},
   "source": [
    "## Load\n",
    "\n",
    "To store our data, we created a relational database using PostgreSQL. For this example, we decided to create one table for each set of data to preserve the existing data structure as much as possible. However, were we to apply this to a future project, we may decide to consolidate to a single  table for greater simplicity.\n",
    "\n",
    "### Creating tables\n",
    "\n",
    "We created our tables using a SQL query, as below. We chose to use `country` as the primary key in each table, as each record in each table is tied to a unique country or group of countries. For a larger dataset, we would have put the country field in its own table and used references to it in each of our tables. However, with only about 230 records in each table, it didn't seem necessary for this exercise, so we chose again to maintain the existing data structure where possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cbe00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Create tables for raw data to be loaded into\n",
    "CREATE TABLE economy_size (\n",
    "    country TEXT PRIMARY KEY,\n",
    "    population_2019 FLOAT,\n",
    "    surface_area_2019 FLOAT,\n",
    "    population_density_2019 FLOAT,\n",
    "    gross_national_income_atlas_method_2019 FLOAT,\n",
    "    gross_national_income_per_capita_atlas_method_2019 FLOAT,\n",
    "    purchasing_power_parity_gross_national_income_2019 FLOAT,\n",
    "    ppp_gni_per_capita_2019 FLOAT,\n",
    "    gross_domestic_product_growth_2019 FLOAT,\n",
    "    per_capita_gdp_growth_2019 FLOAT\n",
    ");\n",
    "\n",
    "CREATE TABLE ending_poverty (\n",
    "    country TEXT PRIMARY KEY,\n",
    "    percentage_share_of_income_or_consumption_2007_18 FLOAT,\n",
    "    prevalence_of_child_malnutrition_2020 FLOAT,\n",
    "    maternal_mortality_ratio_2017 FLOAT,\n",
    "    under_five_mortality_rate_2019 FLOAT,\n",
    "    incidence_of_hiv_ages_15_49_per1000_2020 FLOAT,\n",
    "    incidence_of_tuberculosis_2019 FLOAT,\n",
    "    mortality_caused_by_road_traffic_injury_2016 FLOAT,\n",
    "    primary_completion_rate_2018 FLOAT,\n",
    "    vulnerable_employment_2019 FLOAT,\n",
    "    contributing_family_workers_and_own_account_workers_female FLOAT,\n",
    "    labor_productivity_growth_2015_18 FLOAT\n",
    ");\n",
    "\n",
    "CREATE TABLE promoting_sustainability (\n",
    "    country TEXT PRIMARY KEY,\n",
    "    people_using_safely_managed_drinking_water_services_2017 FLOAT,\n",
    "    people_using_safely_managed_sanitation_services_2017 FLOAT,\n",
    "    access_to_electricity_2017 FLOAT,\n",
    "    renewable_energy_consumption_2015 FLOAT,\n",
    "    expenditures_for_rd_2015 FLOAT,\n",
    "    urban_population_living_in_slums_2014 FLOAT,\n",
    "    ambient_pm2_5_air_pollution_2016 FLOAT,\n",
    "    adjusted_net_savings_2017 FLOAT,\n",
    "    carbon_dioxide_emissions_2014 FLOAT,\n",
    "    nationally_protected_terrestrial_and_marine_areas_2018 FLOAT,\n",
    "    intentional_homicides_2015 FLOAT,\n",
    "    internet_use_2017 FLOAT\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87983444",
   "metadata": {},
   "source": [
    "### Loading data into tables\n",
    "\n",
    "We used an SQAlchemy engine to connect to our database in PostgreSQL. For each table, we appended records where a country did not already exist, and dropped the index because with `country` serving as primary key, it was superfluous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fef99ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create postgres connection\n",
    "\n",
    "rds_connection_string = username + \":\" + password + \"@localhost:5432/world_development_indicators\"\n",
    "engine = create_engine(f'postgresql://{rds_connection_string}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd6b6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eeacca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write data to economy_size table\n",
    "\n",
    "wv1_df2.to_sql(name='economy_size', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5946f4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write data to ending_poverty table\n",
    "\n",
    "wv2_df2.to_sql(name='ending_poverty', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8d1c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write data to promoting_sustainability table\n",
    "\n",
    "wv3_df2.to_sql(name='promoting_sustainability', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce0975b",
   "metadata": {},
   "source": [
    "### Testing with a join\n",
    "\n",
    "To ensure our database functioned as expected, we wrote a join in SQL that would pull columns from each database based on the table's `country` field. This allowed us to be certain that the data fields had populated with the expected values, and that our goal of making it possible to compare datapoints from across the different data sets was accomplished. That code is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbef505",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "-- Query to check successful load\n",
    "SELECT * FROM economy_size;\n",
    "\n",
    "SELECT * FROM ending_poverty;\n",
    "\n",
    "SELECT * FROM promoting_sustainability;\n",
    "\n",
    "-- Join tables\n",
    "SELECT es.country, es.population_2019, es.gross_domestic_product_growth_2019, ep.percentage_share_of_income_or_consumption_2007_18, ep.prevalence_of_child_malnutrition_2020, ep.under_five_mortality_rate_2019, ps.people_using_safely_managed_drinking_water_services_2017, ps.ambient_pm2_5_air_pollution_2016\n",
    "FROM economy_size as es\n",
    "INNER JOIN ending_poverty as ep\n",
    "ON es.country = ep.country\n",
    "INNER JOIN promoting_sustainability as ps\n",
    "ON es.country = ps.country\n",
    ";"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
